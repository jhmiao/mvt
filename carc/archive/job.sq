#!/bin/bash
#SBATCH --job-name=mdp_experiments
#SBATCH --account=ssuen_1436
#SBATCH --output=mdp_experiments_%j.out
#SBATCH --error=mdp_experiments_%j.err
#SBATCH --mail-user=jinjing@usc.edu
#SBATCH --mail-type=ALL
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=8
#SBATCH --partition=main
#SBATCH --time=36:00:00        # or any value ≤ that queue’s limit
#SBATCH --mem=32G

# Load required modules (adjust based on your HPC system)
module load python/3.9
module load numpy
module load pandas

# Activate virtual environment if needed
# source /path/to/your/venv/bin/activate

# Set working directory
cd $SLURM_SUBMIT_DIR

# Install Python dependencies
pip install --user numpy pandas

# Print job information
echo "Job started at: $(date)"
echo "Job ID: $SLURM_JOB_ID"
echo "Running on node: $SLURM_NODELIST"
echo "Working directory: $(pwd)"

# Run the experiment with parallel processing
# 1. Original version with fixed parameters (sequential)
# python experiment_runner.py

# 2. Parallel version - adjust --parallel-jobs to match --cpus-per-task
# Example configurations:
python experiment_runner_hpc.py --locations 3 --slots 5 --days 3 --seeds 42 456 --parallel-jobs 8

echo "Job completed at: $(date)" 